"""
EU AI Act 2025 Analyzer

This module implements compliance checks for the EU AI Act 2025.
It analyzes AI models against the latest European regulatory requirements and provides
detailed compliance assessments and recommendations.

Key features:
- Risk categorization based on AI application purpose
- Mandatory requirements assessment for high-risk AI systems
- Prohibited AI practices detection
- Transparency and documentation compliance checks
- Governance and oversight recommendations
"""

import logging
import json
import os
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple, Set

# Configure logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# EU AI Act 2025 Risk Categories
AI_RISK_CATEGORIES = {
    "prohibited": {
        "name": "Prohibited AI Practice",
        "description": "AI systems that are explicitly prohibited under the EU AI Act",
        "examples": [
            "Social scoring systems by public authorities",
            "Emotion recognition in workplace/educational settings",
            "Biometric categorization based on sensitive attributes",
            "Untargeted scraping of facial images",
            "Predictive policing based solely on profiling"
        ]
    },
    "high_risk": {
        "name": "High-Risk AI System",
        "description": "AI systems with significant potential impact on health, safety, or fundamental rights",
        "examples": [
            "Critical infrastructure management",
            "Educational and vocational training",
            "Employment, worker management, and access to self-employment",
            "Access to essential private/public services",
            "Law enforcement systems",
            "Migration, asylum and border control management",
            "Administration of justice and democratic processes"
        ]
    },
    "general_purpose": {
        "name": "General Purpose AI System",
        "description": "AI systems with general capabilities that can serve various functions",
        "examples": [
            "Large language models",
            "Foundation models",
            "Multimodal AI systems",
            "General-purpose computer vision systems"
        ]
    },
    "limited_risk": {
        "name": "Limited Risk AI System",
        "description": "AI systems with transparency obligations but less stringent requirements",
        "examples": [
            "AI systems that interact with humans",
            "Emotion recognition systems (non-workplace/educational)",
            "Biometric categorization systems (non-sensitive attributes)",
            "Content generation systems (deepfakes)"
        ]
    },
    "minimal_risk": {
        "name": "Minimal Risk AI System",
        "description": "AI systems that pose minimal risk to rights or safety",
        "examples": [
            "AI-enabled video games",
            "Spam filters",
            "Simple recommendation systems",
            "Basic analytics tools"
        ]
    }
}

# EU AI Act 2025 Mandatory Requirements for High-Risk Systems
MANDATORY_REQUIREMENTS = {
    "risk_management": {
        "name": "Risk Management System",
        "description": "Establish a risk management system throughout the entire AI system lifecycle",
        "checks": [
            "Risk identification and analysis",
            "Risk estimation and evaluation",
            "Risk monitoring and review",
            "Risk treatment and mitigation",
            "Risk documentation and reporting"
        ]
    },
    "data_governance": {
        "name": "Data and Data Governance",
        "description": "Implement appropriate data governance and management practices",
        "checks": [
            "Data quality criteria",
            "Data preparation processing",
            "Data examination for biases",
            "Data security measures",
            "Data traceability throughout lifecycle"
        ]
    },
    "technical_documentation": {
        "name": "Technical Documentation",
        "description": "Maintain detailed technical documentation demonstrating compliance",
        "checks": [
            "System description and purpose",
            "Design specifications",
            "Development methodologies",
            "Testing and validation processes",
            "Risk management measures"
        ]
    },
    "record_keeping": {
        "name": "Record-keeping",
        "description": "Maintain logs generated by the high-risk AI system",
        "checks": [
            "Automatic recording of events",
            "Logging of decision processes",
            "Log security and integrity",
            "Traceability of system outputs",
            "Log retention policies"
        ]
    },
    "transparency": {
        "name": "Transparency",
        "description": "Provide clear information to users about the system",
        "checks": [
            "System capabilities and limitations",
            "Purpose and intended use",
            "Human oversight measures",
            "Expected lifetime and maintenance",
            "Performance metrics"
        ]
    },
    "human_oversight": {
        "name": "Human Oversight",
        "description": "Implement appropriate human oversight measures",
        "checks": [
            "Clearly defined roles and responsibilities",
            "Human intervention tools and procedures",
            "Real-time monitoring capabilities",
            "Override mechanisms",
            "Awareness of automation bias"
        ]
    },
    "accuracy_robustness": {
        "name": "Accuracy, Robustness and Cybersecurity",
        "description": "Ensure appropriate levels of accuracy, robustness and cybersecurity",
        "checks": [
            "Accuracy metrics and thresholds",
            "Reliability testing",
            "Resilience to errors and inconsistencies",
            "Resilience to adversarial attacks",
            "Cybersecurity risk assessment"
        ]
    },
    "conformity_assessment": {
        "name": "Conformity Assessment",
        "description": "Undergo conformity assessment procedures",
        "checks": [
            "Internal control documentation",
            "Compliance with harmonized standards",
            "Quality management system",
            "Notification to competent authorities",
            "CE marking procedures"
        ]
    }
}

# GPAI Requirements (for foundational/general purpose models)
GPAI_REQUIREMENTS = {
    "model_evaluation": {
        "name": "Systemic Risk Evaluation",
        "description": "Evaluate and mitigate potential systemic risks",
        "checks": [
            "Capability assessment",
            "Foreseeable risk evaluation",
            "Systemic risk mitigation measures",
            "Safety and performance testing",
            "Red team testing and adversarial evaluation"
        ]
    },
    "documentation": {
        "name": "Technical Documentation",
        "description": "Create and maintain detailed technical documentation",
        "checks": [
            "Model architecture description",
            "Training methodologies",
            "Computational resources utilized",
            "Data governance practices",
            "Performance characteristics"
        ]
    },
    "copyright_compliance": {
        "name": "Copyright Compliance",
        "description": "Demonstrate compliance with copyright law for training data",
        "checks": [
            "Copyright clearance documentation",
            "Model training data sourcing details",
            "Usage agreements for copyrighted material",
            "Measures to prevent unauthorized reproductions",
            "Content attribution mechanisms"
        ]
    },
    "information_disclosure": {
        "name": "Information Disclosure",
        "description": "Publish summaries about content used for training",
        "checks": [
            "Training data overview",
            "Content categories description",
            "Data source transparency",
            "Usage limitations disclosure",
            "Model capabilities and limitations summary"
        ]
    }
}

# Prohibited AI practices under EU AI Act 2025
PROHIBITED_PRACTICES = [
    {
        "id": "social_scoring",
        "name": "Social Scoring by Public Authorities",
        "description": "AI systems that evaluate or classify individuals based on social behavior or characteristics, leading to detrimental treatment in social contexts unrelated to the contexts in which the data was generated",
        "detection_patterns": [
            "Public authority deployment",
            "Social behavior scoring",
            "Citizen evaluation metrics",
            "Cross-context use of social data",
            "Public trust or reliability scoring"
        ]
    },
    {
        "id": "emotion_recognition_workplace",
        "name": "Emotion Recognition in Workplace/Educational Settings",
        "description": "AI systems that use emotion recognition in workplaces or educational institutions for making decisions that affect people's opportunities",
        "detection_patterns": [
            "Workplace emotion analysis",
            "Student emotion monitoring",
            "Affective computing for evaluation",
            "Sentiment analysis for employment decisions",
            "Educational opportunity allocation based on emotional state"
        ]
    },
    {
        "id": "biometric_categorization",
        "name": "Biometric Categorization Based on Sensitive Attributes",
        "description": "AI systems that categorize individuals based on biometric data according to ethnicity, gender, political or sexual orientation, or other protected characteristics",
        "detection_patterns": [
            "Biometric ethnic classification",
            "Gender identification from physical traits",
            "Political orientation prediction from biometrics",
            "Sexual orientation categorization",
            "Religious affiliation prediction from physical features"
        ]
    },
    {
        "id": "facial_scraping",
        "name": "Untargeted Scraping of Facial Images",
        "description": "AI systems that conduct untargeted scraping of facial images from the internet or CCTV footage to create facial recognition databases",
        "detection_patterns": [
            "Mass facial image collection",
            "Public camera network data harvesting",
            "Indiscriminate facial database creation",
            "Internet image scraping for facial recognition",
            "Unauthorized biometric data collection"
        ]
    },
    {
        "id": "predictive_policing",
        "name": "Predictive Policing Based Solely on Profiling",
        "description": "AI systems used by law enforcement that predict crimes based solely on profiling of individuals or assessment of personality traits and characteristics",
        "detection_patterns": [
            "Individual crime prediction",
            "Personality-based risk assessment",
            "Character trait policing models",
            "Profile-based criminality prediction",
            "Behavioral forecasting for law enforcement"
        ]
    },
    {
        "id": "social_manipulation",
        "name": "Manipulative AI Systems",
        "description": "AI systems that deploy subliminal techniques to materially distort behavior in a manner that causes physical or psychological harm",
        "detection_patterns": [
            "Subliminal messaging techniques",
            "Behavior manipulation algorithms",
            "Psychological vulnerability exploitation",
            "Addiction-promoting mechanisms",
            "Harmful distortion of user perception"
        ]
    }
]

def analyze_ai_model(model_details: Dict[str, Any]) -> Dict[str, Any]:
    """
    Analyze an AI model against EU AI Act 2025 requirements
    
    Args:
        model_details: Dictionary containing model information
        
    Returns:
        Dictionary containing compliance analysis results
    """
    logger.info("Starting EU AI Act 2025 compliance analysis")
    
    # Initialize analysis results
    analysis_results = {
        "timestamp": datetime.now().isoformat(),
        "model_name": model_details.get("model_name", "Unnamed Model"),
        "model_type": model_details.get("model_type", "Unknown"),
        "analysis_id": f"EUAI-{datetime.now().strftime('%Y%m%d')}-{os.urandom(3).hex()}",
        "risk_category": "unclassified",
        "risk_category_details": {},
        "compliance_findings": [],
        "prohibited_practice_findings": [],
        "mandatory_requirements_findings": [],
        "gpai_requirements_findings": [],
        "compliance_score": 0,
        "recommendations": []
    }
    
    try:
        # Step 1: Determine risk category
        risk_category, risk_details = _categorize_ai_risk(model_details)
        analysis_results["risk_category"] = risk_category
        analysis_results["risk_category_details"] = risk_details
        
        # Step 2: Check for prohibited practices
        prohibited_findings = _check_prohibited_practices(model_details)
        analysis_results["prohibited_practice_findings"] = prohibited_findings
        
        # Step 3: Evaluate mandatory requirements for high-risk systems
        if risk_category == "high_risk" or risk_category == "prohibited":
            mandatory_findings = _evaluate_mandatory_requirements(model_details)
            analysis_results["mandatory_requirements_findings"] = mandatory_findings
        
        # Step 4: Evaluate GPAI requirements for general purpose AI
        if risk_category == "general_purpose" or model_details.get("is_foundation_model", False):
            gpai_findings = _evaluate_gpai_requirements(model_details)
            analysis_results["gpai_requirements_findings"] = gpai_findings
        
        # Step 5: Generate overall compliance findings
        compliance_findings = _generate_compliance_findings(
            risk_category, 
            prohibited_findings,
            analysis_results.get("mandatory_requirements_findings", []),
            analysis_results.get("gpai_requirements_findings", [])
        )
        analysis_results["compliance_findings"] = compliance_findings
        
        # Step 6: Calculate compliance score
        compliance_score = _calculate_compliance_score(analysis_results)
        analysis_results["compliance_score"] = compliance_score
        
        # Step 7: Generate recommendations
        recommendations = _generate_recommendations(analysis_results)
        analysis_results["recommendations"] = recommendations
        
        logger.info(f"Completed EU AI Act analysis with compliance score: {compliance_score}")
        return analysis_results
        
    except Exception as e:
        logger.error(f"Error during EU AI Act analysis: {str(e)}")
        analysis_results["error"] = str(e)
        analysis_results["compliance_findings"].append({
            "id": "error_during_analysis",
            "category": "Error",
            "description": f"An error occurred during analysis: {str(e)}",
            "severity": "high",
            "impact": "Analysis could not be completed"
        })
        return analysis_results

def _categorize_ai_risk(model_details: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]:
    """
    Categorize an AI model according to EU AI Act risk levels
    
    Args:
        model_details: Dictionary containing model information
        
    Returns:
        Tuple containing risk category and details
    """
    # Default to minimal risk if we can't determine
    risk_category = "minimal_risk"
    confidence = 0.5
    risk_factors = []
    
    # Extract relevant model information
    purpose = model_details.get("purpose", "").lower()
    capabilities = model_details.get("capabilities", [])
    deployment_context = model_details.get("deployment_context", "").lower()
    intended_users = model_details.get("intended_users", [])
    is_foundation_model = model_details.get("is_foundation_model", False)
    is_generative_ai = model_details.get("is_generative_ai", False)
    
    # Check for prohibited applications
    prohibited_keywords = [
        "social scoring", "social credit", "public authority scoring",
        "workplace emotion recognition", "emotion monitoring at work",
        "biometric categorization by race", "ethnic categorization", 
        "face scraping", "mass facial collection",
        "predictive policing", "crime prediction by profile"
    ]
    
    if any(keyword in purpose for keyword in prohibited_keywords):
        risk_category = "prohibited"
        confidence = 0.95
        risk_factors.append("Purpose contains prohibited application keywords")
    
    # Check for high-risk applications
    high_risk_domains = [
        "critical infrastructure", "educational", "education", "school", "university",
        "employment", "recruiting", "hiring", "worker management",
        "essential service", "healthcare", "medical", "law enforcement", "police",
        "justice", "court", "judge", "voting", "migration", "asylum", "border control"
    ]
    
    if any(keyword in purpose for keyword in high_risk_domains) or \
       any(keyword in deployment_context for keyword in high_risk_domains):
        risk_category = "high_risk"
        confidence = 0.9
        risk_factors.append("Purpose or deployment context includes high-risk domain")
    
    # Check for general purpose AI
    if is_foundation_model or is_generative_ai or \
       "general purpose" in purpose or \
       "foundation model" in purpose:
        risk_category = "general_purpose"
        confidence = 0.85
        risk_factors.append("Model is identified as foundation model or general purpose AI")
    
    # Check for limited risk AI systems
    limited_risk_indicators = [
        "chatbot", "virtual assistant", "customer service",
        "human interaction", "emotion recognition",
        "deepfake", "synthetic media", "content generation"
    ]
    
    if any(indicator in purpose for indicator in limited_risk_indicators) and risk_category == "minimal_risk":
        risk_category = "limited_risk"
        confidence = 0.7
        risk_factors.append("Purpose includes limited risk application indicators")
    
    # Prepare detailed risk information
    risk_details = {
        "category": risk_category,
        "name": AI_RISK_CATEGORIES[risk_category]["name"],
        "description": AI_RISK_CATEGORIES[risk_category]["description"],
        "confidence": confidence,
        "risk_factors": risk_factors,
        "evaluation_date": datetime.now().isoformat()
    }
    
    return risk_category, risk_details

def _check_prohibited_practices(model_details: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Check if model involves prohibited AI practices under EU AI Act
    
    Args:
        model_details: Dictionary containing model information
        
    Returns:
        List of prohibited practice findings
    """
    findings = []
    
    # Extract relevant model information
    purpose = model_details.get("purpose", "").lower()
    capabilities = model_details.get("capabilities", [])
    training_data = model_details.get("training_data", {})
    deployment_context = model_details.get("deployment_context", "").lower()
    
    # Check each prohibited practice
    for practice in PROHIBITED_PRACTICES:
        practice_id = practice["id"]
        match_count = 0
        matched_patterns = []
        
        # Check detection patterns against model details
        for pattern in practice["detection_patterns"]:
            pattern_lower = pattern.lower()
            
            # Look for pattern matches in various model details
            if pattern_lower in purpose or \
               pattern_lower in str(capabilities).lower() or \
               pattern_lower in deployment_context or \
               any(pattern_lower in str(data).lower() for data in training_data.values()):
                match_count += 1
                matched_patterns.append(pattern)
        
        # If we have matches, add a finding
        if match_count > 0:
            confidence = min(0.95, match_count / len(practice["detection_patterns"]) * 0.9)
            
            findings.append({
                "id": f"prohibited_{practice_id}",
                "practice_id": practice_id,
                "name": practice["name"],
                "description": practice["description"],
                "matched_patterns": matched_patterns,
                "confidence": confidence,
                "severity": "critical",
                "remediation": f"This AI system may involve prohibited practices under EU AI Act Article 5. "
                              f"Review and redesign the system to avoid {practice['name'].lower()}."
            })
    
    return findings

def _evaluate_mandatory_requirements(model_details: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Evaluate mandatory requirements for high-risk AI systems
    
    Args:
        model_details: Dictionary containing model information
        
    Returns:
        List of requirement compliance findings
    """
    findings = []
    
    # Get model documentation details
    documentation = model_details.get("documentation", {})
    testing_info = model_details.get("testing", {})
    data_governance = model_details.get("data_governance", {})
    oversight = model_details.get("human_oversight", {})
    
    # Check each mandatory requirement
    for req_id, requirement in MANDATORY_REQUIREMENTS.items():
        requirement_name = requirement["name"]
        requirement_findings = []
        
        # Check for evidence of each aspect of the requirement
        for check in requirement["checks"]:
            check_id = check.lower().replace(" ", "_")
            check_result = {
                "id": f"{req_id}_{check_id}",
                "name": check,
                "compliant": False,
                "evidence": None,
                "recommendation": f"Implement {check.lower()} as part of {requirement_name.lower()}"
            }
            
            # Look for evidence in model details
            # This would be much more sophisticated in a real implementation
            if req_id == "risk_management" and any(check.lower() in str(key).lower() for key in documentation):
                check_result["compliant"] = True
                check_result["evidence"] = f"Found evidence of {check.lower()} in documentation"
            
            elif req_id == "data_governance" and any(check.lower() in str(key).lower() for key in data_governance):
                check_result["compliant"] = True
                check_result["evidence"] = f"Found evidence of {check.lower()} in data governance documentation"
            
            elif req_id == "technical_documentation" and any(check.lower() in str(key).lower() for key in documentation):
                check_result["compliant"] = True
                check_result["evidence"] = f"Found evidence of {check.lower()} in technical documentation"
            
            elif req_id == "record_keeping" and "logs" in str(documentation).lower():
                check_result["compliant"] = True
                check_result["evidence"] = f"Found evidence of {check.lower()} in logging documentation"
            
            elif req_id == "transparency" and "user_information" in str(documentation).lower():
                check_result["compliant"] = True
                check_result["evidence"] = f"Found evidence of {check.lower()} in user information documentation"
            
            elif req_id == "human_oversight" and any(check.lower() in str(key).lower() for key in oversight):
                check_result["compliant"] = True
                check_result["evidence"] = f"Found evidence of {check.lower()} in human oversight documentation"
            
            elif req_id == "accuracy_robustness" and any(check.lower() in str(key).lower() for key in testing_info):
                check_result["compliant"] = True
                check_result["evidence"] = f"Found evidence of {check.lower()} in testing documentation"
            
            elif req_id == "conformity_assessment" and "conformity" in str(documentation).lower():
                check_result["compliant"] = True
                check_result["evidence"] = f"Found evidence of {check.lower()} in conformity assessment documentation"
            
            requirement_findings.append(check_result)
        
        # Calculate compliance percentage for this requirement
        compliant_checks = sum(1 for check in requirement_findings if check["compliant"])
        compliance_percentage = compliant_checks / len(requirement_findings) if requirement_findings else 0
        
        # Add overall finding for this requirement
        findings.append({
            "id": req_id,
            "name": requirement_name,
            "description": requirement["description"],
            "compliance_percentage": compliance_percentage,
            "compliant": compliance_percentage >= 0.8,  # 80% threshold for overall compliance
            "checks": requirement_findings,
            "severity": "high" if compliance_percentage < 0.5 else "medium" if compliance_percentage < 0.8 else "low"
        })
    
    return findings

def _evaluate_gpai_requirements(model_details: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Evaluate requirements for General Purpose AI systems
    
    Args:
        model_details: Dictionary containing model information
        
    Returns:
        List of GPAI requirement compliance findings
    """
    findings = []
    
    # Get model documentation details
    documentation = model_details.get("documentation", {})
    training_info = model_details.get("training", {})
    evaluation_info = model_details.get("evaluation", {})
    
    # Check each GPAI requirement
    for req_id, requirement in GPAI_REQUIREMENTS.items():
        requirement_name = requirement["name"]
        requirement_description = requirement["description"]
        requirement_findings = []
        
        # Check for evidence of each aspect of the requirement
        for check in requirement["checks"]:
            check_id = check.lower().replace(" ", "_")
            check_result = {
                "id": f"{req_id}_{check_id}",
                "name": check,
                "compliant": False,
                "evidence": None,
                "recommendation": f"Implement {check.lower()} as part of {requirement_name.lower()} for GPAI compliance"
            }
            
            # Look for evidence in model details
            # Mock evaluation based on model details - in a real system, this would be more sophisticated
            if req_id == "model_evaluation":
                # Check if evaluation info contains keywords related to this check
                evaluation_keys = " ".join(str(key).lower() for key in evaluation_info.keys())
                if any(keyword in check.lower() for keyword in ["risk", "evaluation", "assessment"]):
                    has_risk_assessment = "risk" in evaluation_keys or "assessment" in evaluation_keys
                    check_result["compliant"] = has_risk_assessment
                    if has_risk_assessment:
                        check_result["evidence"] = f"Found evidence of {check.lower()} in model evaluation documentation"
                elif "test" in check.lower():
                    has_testing = "test" in evaluation_keys or "safety" in evaluation_keys
                    check_result["compliant"] = has_testing
                    if has_testing:
                        check_result["evidence"] = f"Found safety and performance testing documentation"
                else:
                    # Random check for demo purposes
                    check_result["compliant"] = len(evaluation_info) > 0
                    if check_result["compliant"]:
                        check_result["evidence"] = f"Found general evaluation documentation that may support {check.lower()}"
                    
            elif req_id == "documentation":
                # Check if documentation contains relevant information
                doc_keys = " ".join(str(key).lower() for key in documentation.keys())
                if "architecture" in check.lower() and "model_card" in documentation:
                    check_result["compliant"] = True
                    check_result["evidence"] = "Model architecture description found in model card"
                elif "training" in check.lower() and "data_sheets" in documentation:
                    check_result["compliant"] = True
                    check_result["evidence"] = "Training methodology documentation found in data sheets"
                elif any(keyword in check.lower() for keyword in ["resource", "computational"]):
                    check_result["compliant"] = "technical_docs" in documentation
                    if check_result["compliant"]:
                        check_result["evidence"] = "Computational resource documentation found in technical docs"
                else:
                    # Default check for any documentation
                    check_result["compliant"] = len(documentation) > 0
                    if check_result["compliant"]:
                        check_result["evidence"] = f"Found documentation that may cover {check.lower()}"
            
            elif req_id == "copyright_compliance":
                # Check for copyright-related information
                training_data_str = str(training_info).lower()
                if "copyright" in check.lower():
                    has_copyright = "copyright" in training_data_str or "license" in training_data_str
                    check_result["compliant"] = has_copyright
                    if has_copyright:
                        check_result["evidence"] = "Copyright documentation found in training information"
                elif "source" in check.lower():
                    has_sources = "data" in training_data_str and "source" in training_data_str
                    check_result["compliant"] = has_sources
                    if has_sources:
                        check_result["evidence"] = "Training data source documentation found"
                else:
                    # Default check for any training info
                    check_result["compliant"] = len(training_info) > 0
                    if check_result["compliant"]:
                        check_result["evidence"] = f"Found training documentation that may address {check.lower()}"
            
            elif req_id == "information_disclosure":
                # Check for information disclosure documentation
                doc_str = str(documentation).lower()
                if "data" in check.lower() or "training" in check.lower():
                    has_data_info = "data" in doc_str and "training" in doc_str
                    check_result["compliant"] = has_data_info
                    if has_data_info:
                        check_result["evidence"] = "Training data overview found in documentation"
                elif "limitation" in check.lower():
                    has_limitations = "limitation" in doc_str or "constraint" in doc_str
                    check_result["compliant"] = has_limitations
                    if has_limitations:
                        check_result["evidence"] = "Usage limitations documentation found"
                else:
                    # Default check for disclosure info
                    check_result["compliant"] = "disclosure" in doc_str or "transparency" in doc_str
                    if check_result["compliant"]:
                        check_result["evidence"] = f"Found disclosure documentation related to {check.lower()}"
            
            # Add detailed recommendation if not compliant
            if not check_result["compliant"]:
                if req_id == "model_evaluation":
                    check_result["recommendation"] = f"Implement {check} with documented evidence of system behavior assessments and risk mitigation strategies"
                elif req_id == "documentation":
                    check_result["recommendation"] = f"Create detailed {check.lower()} within your technical documentation that meets EU AI Act transparency requirements"
                elif req_id == "copyright_compliance":
                    check_result["recommendation"] = f"Document {check.lower()} to demonstrate lawful acquisition and use of training data"
                elif req_id == "information_disclosure":
                    check_result["recommendation"] = f"Publish {check.lower()} to ensure transparency about model capabilities and limitations"
            
            requirement_findings.append(check_result)
        
        # Calculate compliance percentage for this requirement
        compliant_checks = sum(1 for check in requirement_findings if check["compliant"])
        compliance_percentage = compliant_checks / len(requirement_findings) if requirement_findings else 0
        
        # Add overall finding for this requirement
        findings.append({
            "id": req_id,
            "name": requirement_name,
            "description": requirement_description,
            "compliance_percentage": compliance_percentage,
            "compliant": compliance_percentage >= 0.8,  # 80% threshold for overall compliance
            "checks": requirement_findings,
            "severity": "high" if compliance_percentage < 0.5 else "medium" if compliance_percentage < 0.8 else "low"
        })
    
    return findings

def _generate_compliance_findings(
    risk_category: str,
    prohibited_findings: List[Dict[str, Any]], 
    mandatory_findings: List[Dict[str, Any]],
    gpai_findings: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Generate overall compliance findings
    
    Args:
        risk_category: Risk category of the AI system
        prohibited_findings: Findings related to prohibited practices
        mandatory_findings: Findings related to mandatory requirements
        gpai_findings: Findings related to GPAI requirements
        
    Returns:
        List of overall compliance findings
    """
    findings = []
    
    # Add finding for risk categorization
    findings.append({
        "id": "risk_categorization",
        "category": "Risk Level",
        "description": f"System categorized as {AI_RISK_CATEGORIES[risk_category]['name']}",
        "details": AI_RISK_CATEGORIES[risk_category]['description'],
        "severity": "critical" if risk_category == "prohibited" else 
                   "high" if risk_category == "high_risk" else 
                   "medium" if risk_category == "general_purpose" else 
                   "low" if risk_category == "limited_risk" else "info"
    })
    
    # Add findings for prohibited practices
    if prohibited_findings:
        for finding in prohibited_findings:
            findings.append({
                "id": f"prohibited_{finding['practice_id']}",
                "category": "Prohibited Practice",
                "description": f"Potential {finding['name']} detected",
                "details": finding['description'],
                "matched_patterns": finding.get('matched_patterns', []),
                "confidence": finding.get('confidence', 0.5),
                "severity": "critical",
                "remediation": finding.get('remediation', 'Redesign system to avoid this prohibited practice')
            })
    else:
        findings.append({
            "id": "no_prohibited_practices",
            "category": "Prohibited Practices",
            "description": "No prohibited practices detected",
            "severity": "info",
            "details": "The system does not appear to implement any practices prohibited under the EU AI Act"
        })
    
    # Add findings for mandatory requirements if high-risk
    if risk_category in ["high_risk", "prohibited"]:
        non_compliant_requirements = [req for req in mandatory_findings if not req["compliant"]]
        
        if non_compliant_requirements:
            for req in non_compliant_requirements:
                findings.append({
                    "id": f"mandatory_{req['id']}",
                    "category": "Mandatory Requirement",
                    "description": f"{req['name']} requirement not fully met",
                    "details": f"{req['description']} - Current compliance: {req['compliance_percentage']:.0%}",
                    "severity": req["severity"],
                    "checks": req.get("checks", [])
                })
        else:
            findings.append({
                "id": "mandatory_requirements_met",
                "category": "Mandatory Requirements",
                "description": "All high-risk AI mandatory requirements met",
                "severity": "info"
            })
    
    # Add findings for GPAI requirements if applicable
    if risk_category == "general_purpose" and gpai_findings:
        non_compliant_requirements = [req for req in gpai_findings if not req["compliant"]]
        
        if non_compliant_requirements:
            for req in non_compliant_requirements:
                findings.append({
                    "id": f"gpai_{req['id']}",
                    "category": "GPAI Requirement",
                    "description": f"{req['name']} requirement not fully met",
                    "details": f"{req['description']} - Current compliance: {req['compliance_percentage']:.0%}",
                    "severity": req["severity"],
                    "checks": req.get("checks", [])
                })
        else:
            findings.append({
                "id": "gpai_requirements_met",
                "category": "GPAI Requirements",
                "description": "All General Purpose AI requirements met",
                "severity": "info"
            })
    
    return findings

def _calculate_compliance_score(analysis_results: Dict[str, Any]) -> float:
    """
    Calculate overall compliance score based on findings
    
    Args:
        analysis_results: Complete analysis results
        
    Returns:
        Compliance score from 0 to 100
    """
    # Start with base score
    base_score = 100
    
    # Deduct points for prohibited practices
    prohibited_practices = analysis_results.get("prohibited_practice_findings", [])
    if prohibited_practices:
        base_score -= len(prohibited_practices) * 25  # -25 points per prohibited practice
    
    # Deduct points for mandatory requirements (high-risk systems)
    if analysis_results["risk_category"] in ["high_risk", "prohibited"]:
        mandatory_requirements = analysis_results.get("mandatory_requirements_findings", [])
        for req in mandatory_requirements:
            if not req.get("compliant", False):
                compliance_percentage = req.get("compliance_percentage", 0)
                # Deduct 5-15 points based on compliance percentage
                deduction = int((1 - compliance_percentage) * 15)
                base_score -= deduction
    
    # Deduct points for GPAI requirements
    if analysis_results["risk_category"] == "general_purpose":
        gpai_requirements = analysis_results.get("gpai_requirements_findings", [])
        for req in gpai_requirements:
            if not req.get("compliant", False):
                compliance_percentage = req.get("compliance_percentage", 0)
                # Deduct 5-10 points based on compliance percentage
                deduction = int((1 - compliance_percentage) * 10)
                base_score -= deduction
    
    # Ensure score is between 0 and 100
    return max(0, min(100, base_score))

def _generate_recommendations(analysis_results: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Generate compliance recommendations based on analysis results
    
    Args:
        analysis_results: Complete analysis results
        
    Returns:
        List of recommendations
    """
    recommendations = []
    risk_category = analysis_results["risk_category"]
    
    # Recommendations for prohibited practices
    prohibited_findings = analysis_results.get("prohibited_practice_findings", [])
    for finding in prohibited_findings:
        recommendations.append({
            "id": f"rec_prohibited_{finding['practice_id']}",
            "priority": "critical",
            "category": "Prohibited Practice",
            "title": f"Redesign system to eliminate {finding['name']}",
            "description": f"This system may implement {finding['name']}, which is prohibited under EU AI Act Article 5.",
            "action_items": [
                "Immediately halt development or deployment if in production",
                f"Remove all functionality related to {finding['name']}",
                "Conduct a thorough review of system purpose and implementation",
                "Consult legal counsel for detailed compliance guidance"
            ]
        })
    
    # Recommendations for high-risk systems
    if risk_category in ["high_risk", "prohibited"]:
        mandatory_findings = analysis_results.get("mandatory_requirements_findings", [])
        for finding in mandatory_findings:
            if not finding.get("compliant", False):
                # Get non-compliant checks
                non_compliant_checks = [check for check in finding.get("checks", []) if not check.get("compliant", False)]
                action_items = [check.get("recommendation") for check in non_compliant_checks]
                
                recommendations.append({
                    "id": f"rec_mandatory_{finding['id']}",
                    "priority": finding.get("severity", "medium"),
                    "category": "Mandatory Requirement",
                    "title": f"Implement {finding['name']} requirement",
                    "description": f"{finding['description']}. Current compliance: {finding['compliance_percentage']:.0%}",
                    "action_items": action_items
                })
    
    # Recommendations for GPAI systems
    if risk_category == "general_purpose":
        gpai_findings = analysis_results.get("gpai_requirements_findings", [])
        for finding in gpai_findings:
            if not finding.get("compliant", False):
                # Get non-compliant checks
                non_compliant_checks = [check for check in finding.get("checks", []) if not check.get("compliant", False)]
                action_items = [check.get("recommendation") for check in non_compliant_checks]
                
                recommendations.append({
                    "id": f"rec_gpai_{finding['id']}",
                    "priority": finding.get("severity", "medium"),
                    "category": "GPAI Requirement",
                    "title": f"Implement {finding['name']} requirement",
                    "description": f"{finding['description']}. Current compliance: {finding['compliance_percentage']:.0%}",
                    "action_items": action_items
                })
    
    # General recommendations based on risk category
    recommendations.append({
        "id": "rec_general_compliance",
        "priority": "high" if risk_category in ["high_risk", "prohibited", "general_purpose"] else "medium",
        "category": "General Compliance",
        "title": f"Maintain compliance documentation for {AI_RISK_CATEGORIES[risk_category]['name']}",
        "description": "Ensure comprehensive documentation is maintained throughout the AI system lifecycle",
        "action_items": [
            "Document design decisions and risk mitigation measures",
            "Maintain records of training data sources and preparation methods",
            "Implement version control for model and code changes",
            "Document testing procedures and results",
            "Establish a compliance monitoring and review process"
        ]
    })
    
    return recommendations